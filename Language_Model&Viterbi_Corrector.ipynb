{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "from collections import Counter\n",
    "from nltk.corpus import europarl_raw\n",
    "from nltk import ngrams\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import words\n",
    "\n",
    "# The nltk version is 3.2.2.\n",
    "# The scikit-learn version is 0.18.1.\n",
    "#corpus size 19899 sentences\n",
    "#alpha value is chosen using the validation set\n",
    "alpha = 0.023 \n",
    "corpus = europarl_raw.english\n",
    "# Delta estimation based on Good Turing\n",
    "delta = 0.75\n",
    "P_cont_denom_bigrams = 0\n",
    "\n",
    "#split(sentences) into train-->60% validation-->20% and test-->20%\n",
    "train_init, test = train_test_split(corpus.sents(), train_size = 0.8,random_state=4542)\n",
    "train, validation = train_test_split(train_init, train_size = 0.75,random_state=4572)\n",
    "\n",
    "def getCount(countdict,key):\n",
    "\n",
    "    '''Helper function that returns # of uni/bi/tri -grams(key) occurencies in corpus.\n",
    "    If there are not such occurencies in corpus returns 0\n",
    "    '''\n",
    "    try:\n",
    "        return countdict[key]\n",
    "    except KeyError:\n",
    "        return 0  \n",
    "    \n",
    "cnt = Counter()\n",
    "for word in [item for sublist in train for item in sublist]:\n",
    "    cnt[word] += 1\n",
    "\n",
    "#Remove rare words \n",
    "unigrams = { k:v for k, v in cnt.items() if v >= 10}\n",
    "unigrams['<s>'] = 2*len(train) #every sentence starts and ends with <s> symbol\n",
    "unigrams_size = sum(v for v in unigrams.values())\n",
    "V = len(unigrams)\n",
    "\n",
    "\n",
    "bigrams =  (ngram for sent in train for ngram in ngrams(sent, 2,\n",
    "    pad_left=True, pad_right=True, left_pad_symbol='<s>',right_pad_symbol='<s>'))\n",
    "\n",
    "cnt2 = Counter()\n",
    "for bigram in bigrams:\n",
    "    cnt2[bigram] += 1\n",
    "\n",
    "\n",
    "    \n",
    "#remove bigrams containing rare words\n",
    "bigrams_final = { k:v for k, v in cnt2.items() if ((k[0] in unigrams.keys()) and(k[1] in unigrams.keys())) }\n",
    "bigrams_size = sum(v for v in bigrams_final.values())\n",
    "\n",
    "#Auxiliary data structure for bigrams that  used in order to improve Knesser-Ney performance\n",
    "bigrams_KN ={}\n",
    "for k, v in bigrams_final.items():\n",
    "    bigrams_KN[k[0]] = {}\n",
    "for k, v in bigrams_final.items():\n",
    "    bigrams_KN[k[0]][k[1]] = v\n",
    "\n",
    "#Auxiliary data structure for bigrams that used in order to improve Knesser-Ney performance  \n",
    "bigrams_KN_reverse ={}\n",
    "for k, v in bigrams_final.items():\n",
    "    bigrams_KN_reverse[k[1]] = {}\n",
    "for k, v in bigrams_final.items():\n",
    "    bigrams_KN_reverse[k[1]][k[0]] = v\n",
    "\n",
    "#compute P(continuation) denominator for bigrams\n",
    "wordsNum = []\n",
    "for k,v in bigrams_final.items():\n",
    "    wordsNum.append(k[0])\n",
    "P_cont_denom_bigrams = len(set(wordsNum))\n",
    "trigrams = (ngram for sent in train for ngram in ngrams(sent, 3,\n",
    "            pad_left=True, pad_right=True, left_pad_symbol='<s>',right_pad_symbol='<s>'))\n",
    "\n",
    "\n",
    "cnt3 = Counter()\n",
    "for trigram in trigrams:\n",
    "    cnt3[trigram] += 1\n",
    "\n",
    "#remove trigrams containing rare words\n",
    "trigrams_final = { k:v for k, v in cnt3.items() if ((k[0] in unigrams.keys())\n",
    "    and(k[1] in unigrams.keys()) and(k[2] in unigrams.keys())) }\n",
    "trigrams_size = sum(v for v in trigrams_final.values())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def unigram_logprob_ls(unigram):\n",
    "\n",
    "    # laplace smoothing\n",
    "    return math.log2(((getCount(unigrams,unigram))+alpha) / (unigrams_size+(alpha*V)) )\n",
    "\n",
    "def bigram_logprob_ls(bigram):\n",
    "\n",
    "    # laplace smoothing\n",
    "    return math.log2(((getCount(bigrams_final,bigram))+alpha ) / ((getCount(unigrams,(bigram[0])))+(alpha*V))) \n",
    "    \n",
    "def bigram_logprob_Mod_KN(bigram):\n",
    "\n",
    "    # Modified Knesser ney \n",
    "    try:\n",
    "        return math.log2(((max((getCount(bigrams_final,bigram) -delta),0)) / getCount(unigrams,(bigram[0]))) \n",
    "                     +((bigram_Mod_KN_lamda(bigram[0])) * bigram_Mod_KN_Pcont(bigram[1])))\n",
    "    \n",
    "    except (ZeroDivisionError,ValueError):\n",
    "        '''return bigram prob with laplace smoothing if both bigram does not exist\n",
    "        and P(continuation) or/and lamda is  zero\n",
    "        '''\n",
    "        return bigram_logprob_ls(bigram)\n",
    "    \n",
    "    \n",
    "def bigram_Mod_KN_lamda(word):\n",
    "\n",
    "    '''Compute Modified Knesser ney  interpolation term(lamda)\n",
    "    Compute The number of word types that can follow word\n",
    "    '''\n",
    "    try:\n",
    "        times = len(bigrams_KN[word])\n",
    "    except KeyError:\n",
    "        times =  0\n",
    "    lamda = (delta/getCount(unigrams,word)) * times\n",
    "    return lamda\n",
    "\n",
    "def bigram_Mod_KN_Pcont(word):\n",
    "\n",
    "    '''Compute P continuation\n",
    "    Count distinct vocabulary words seen to proceede word\n",
    "    '''\n",
    "    try:\n",
    "        count = len(bigrams_KN_reverse[word])\n",
    "    except KeyError:\n",
    "        count = 0\n",
    "    return count/P_cont_denom_bigrams\n",
    "    \n",
    "\n",
    "def trigram_logprob_ls(trigram):\n",
    "\n",
    "    # laplace smoothing\n",
    "    return math.log2(((getCount(trigrams_final,trigram)) +alpha ) /\n",
    "                     ((getCount(bigrams_final,(trigram[0],trigram[1])))+(alpha*V)))   \n",
    "\n",
    "def logprob_sentence_bigram(sentence):\n",
    "    \n",
    "    sumprob = 0\n",
    "    for i in range(len(sentence)-1):\n",
    "        sumprob += bigram_logprob_ls((sentence[i],sentence[i+1]))\n",
    "    return sumprob\n",
    "\n",
    "def logprob_sentence_bigram_Mod_KN(sentence):\n",
    "    \n",
    "    sumprob = 0\n",
    "    for i in range(len(sentence)-1):\n",
    "        sumprob += bigram_logprob_Mod_KN((sentence[i],sentence[i+1]))\n",
    "    return sumprob       \n",
    "    \n",
    "def logprob_sentence_trigram(sentence):\n",
    "    \n",
    "    sumprob = 0\n",
    "    for i in range(len(sentence)-2):\n",
    "        sumprob += trigram_logprob_ls((sentence[i],sentence[i+1],sentence[i+2]))\n",
    "    return sumprob\n",
    "\n",
    "\n",
    "    \n",
    "def bigram_model_perplexity(corpus):\n",
    "    \n",
    "    #Compute cross entropy and perplexity of our bigram model\n",
    "    sumprob = 0\n",
    "    bigram_count = 0\n",
    "    for sentence in corpus:\n",
    "        sentence = ['<s>'] + sentence + ['<s>'] \n",
    "        bigram_count += (len(sentence) -1)\n",
    "        sumprob += logprob_sentence_bigram(sentence)\n",
    "    cross_entropy = -sumprob/bigram_count\n",
    "    perpl = math.pow(2,cross_entropy)\n",
    "    return cross_entropy,perpl\n",
    "\n",
    "\n",
    "def bigram_model_perplexity_Mod_KN(corpus):\n",
    "    \n",
    "    #Compute cross entropy and perplexity of our bigram model with Modified Knesser-Ney Smoothing\n",
    "    sumprob = 0\n",
    "    bigram_count = 0\n",
    "    for sentence in corpus:\n",
    "        sentence = ['<s>'] + sentence + ['<s>'] \n",
    "        bigram_count += (len(sentence) -1)\n",
    "        sumprob += logprob_sentence_bigram_Mod_KN(sentence)\n",
    "    cross_entropy = -sumprob/bigram_count\n",
    "    perpl = math.pow(2,cross_entropy)\n",
    "    return cross_entropy,perpl\n",
    "\n",
    "def trigram_model_perplexity(corpus):\n",
    "    \n",
    "    #Compute cross entropy and perplexity of our trigram model\n",
    "    sumprob = 0\n",
    "    trigram_count = 0\n",
    "    for sentence in corpus:\n",
    "        sentence = ['<s>','<s>'] + sentence + ['<s>','<s>'] \n",
    "        trigram_count += (len(sentence) -2)\n",
    "        sumprob += logprob_sentence_trigram(sentence)\n",
    "    cross_entropy = -sumprob/trigram_count\n",
    "    perpl = math.pow(2,cross_entropy)\n",
    "    return cross_entropy,perpl\n",
    "    \n",
    "def score_sentence_vs_random(corpus):\n",
    "    \n",
    "    sent= random.choice(corpus)\n",
    "    sent = ['<s>','<s>'] + sent + ['<s>','<s>'] \n",
    "    score_sent = logprob_sentence_trigram(sent)\n",
    "    print(sent,\"score:\",score_sent)\n",
    "    random_sent = []\n",
    "    for i in range(len(sent)-4):\n",
    "        random_sent.append(random.choice(words.words()))\n",
    "    random_sent =['<s>','<s>']+ random_sent + ['<s>','<s>']\n",
    "    score_random_sent = logprob_sentence_trigram(random_sent)\n",
    "    print(random_sent,\"score:\",score_random_sent)\n",
    "\n",
    "def predict_next_word(sentence):\n",
    "    \n",
    "    #predict next word based on most frequent relevant trigrams/ bigrams\n",
    "    suggestions = []\n",
    "    if (len(sentence) >= 2):\n",
    "        trigrams = { k:v for k, v in trigrams_final.items() if ((k[0] == sentence[-2])\n",
    "            and(k[1] == sentence[-1]))  }\n",
    "        if (len(trigrams)>0):\n",
    "            sortdict = [(k, trigrams[k]) for k in sorted(trigrams, key=trigrams.get, reverse=True)]\n",
    "            for k,v in sortdict[:3]:\n",
    "                suggestions.append(k[2])\n",
    "            \n",
    "    else:\n",
    "        bigrams = { k:v for k, v in bigrams_final.items() if ((k[0] == sentence[-1])) }\n",
    "        if (len(bigrams)>0):\n",
    "            sortdict = [(k, bigrams[k]) for k in sorted(bigrams, key=bigrams.get, reverse=True)]\n",
    "            for k,v in sortdict[:3]:\n",
    "                suggestions.append(k[1])\n",
    "    \n",
    "    return suggestions\n",
    "       \n",
    "def bigram_trigram_interpolation_prob(trigram,lamda1,lamda2):\n",
    "    \n",
    "    #interpolate bigrams and trigrams\n",
    "    assert(lamda1+lamda2==1.0),\"error interpolation coefs must sum to 1!\"\n",
    "    return ((lamda1*trigram_logprob_ls((trigram[0],trigram[1],trigram[2]))) + \n",
    "             (lamda2*bigram_logprob_ls((trigram[1],trigram[2]))))\n",
    "\n",
    "def interpolated_sentence(sentence,lamda1,lamda2):\n",
    "    \n",
    "    #propability of sentence using the interpolation of the bigram/trigram models\n",
    "    sumprob = 0\n",
    "    for i in range(len(sentence)-2):\n",
    "        sumprob += bigram_trigram_interpolation_prob((sentence[i],sentence[i+1],sentence[i+2]),lamda1,lamda2)\n",
    "    return sumprob\n",
    "\n",
    "def interpolated_model_perplexity(corpus,lamda1,lamda2):\n",
    "    \n",
    "    #Compute cross entropy and perplexity of our trigram model\n",
    "    sumprob = 0\n",
    "    trigram_count = 0\n",
    "    for sentence in corpus:\n",
    "        sentence = ['<s>','<s>'] + sentence + ['<s>','<s>'] \n",
    "        trigram_count += (len(sentence) -2)\n",
    "        sumprob += interpolated_sentence(sentence,lamda1,lamda2)\n",
    "    cross_entropy = -sumprob/trigram_count\n",
    "    perpl = math.pow(2,cross_entropy)\n",
    "    return perpl\n",
    "\n",
    "#Use validation data to tune alpha parameter\n",
    "# for i in range(1,101,1):\n",
    "#     alpha = i/1000\n",
    "#     print(\"alpha:\", alpha)\n",
    "#     print(bigram_model_perplexity(validation))\n",
    "\n",
    "#     print(trigram_model_perplexity(validation))\n",
    "\n",
    "\n",
    "# for i in range(0, 101, 1):\n",
    "#     lamda1 = i/100\n",
    "#     lamda2 = 1-lamda1\n",
    "#     print(\"lamda1: \",lamda1,\" lamda2: \",lamda2,\" perplexity--> \"\n",
    "#           ,interpolated_model_perplexity(validation,lamda1,lamda2))\n",
    "#Best model where lamda1=0 and lamda2 =1\n",
    "    \n",
    "\n",
    "\n",
    "print(bigram_model_perplexity_Mod_KN(train))\n",
    "print(bigram_model_perplexity(train))\n",
    "print(trigram_model_perplexity(train))\n",
    "print(bigram_model_perplexity_Mod_KN(test))\n",
    "print(bigram_model_perplexity(test))\n",
    "print(trigram_model_perplexity(test))\n",
    "\n",
    "score_sentence_vs_random(test)\n",
    "print(predict_next_word([\"treaty\"]))\n",
    "\n",
    "#Results @ TRAIN(Cross Entropy, perplexity)\n",
    "# (6.069444114679039, 67.15598686977955)--> bigram Knesser-Kney \n",
    "# (6.833099295053231, 114.01653831970505)--> bigram laplace smoothing \n",
    "# (6.198075421538283, 73.41868764893984)--> trigram laplace smoothing\n",
    "\n",
    "#Results @ TEST(Cross Entropy, perplexity)\n",
    "# (6.339900707297582, 81.00284684531835)--> bigram Knesser-Kney \n",
    "# (7.666218268695851, 203.1241926649754)--> bigram laplace smoothing\n",
    "# (8.34177441486956, 324.4324722136551) --> trigram laplace smoothing\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pqdict import pqdict\n",
    "CLOSEST_WORDS = 5\n",
    "VOCABULARY = unigrams.keys()\n",
    "\n",
    "\n",
    "def DP_table(word1,word2):\n",
    "    \n",
    "    ''' Construct DP table,in order to compute Levenstein distance\n",
    "     Use a list of lists as data structure for DP table\n",
    "    '''\n",
    "    dist_table = []\n",
    "    word2_list = list(word2)\n",
    "    for i in range(len(word2)+2):\n",
    "        table_row = []\n",
    "        if (i == 0):\n",
    "            table_row.append(\"\")\n",
    "            table_row.append(\"#\")\n",
    "            table_row.extend(list(word1))\n",
    "        elif (i == 1):\n",
    "            table_row.append(\"#\")\n",
    "            table_row.extend(range(len(word1)+1))\n",
    "        else:\n",
    "            table_row.append(word2_list[i-2])\n",
    "            table_row.append(i-1)\n",
    "            table_row.extend([0]*len(word1))\n",
    "        dist_table.append(table_row)\n",
    "   \n",
    "    return(dist_table)\n",
    "\n",
    "\n",
    "def compute_distance(word1,word2):\n",
    "    \n",
    "    '''fill row by row the DP table with the calculated Levenstein distance\n",
    "    according to the associated cost below\n",
    "    '''\n",
    "    insertion = 1\n",
    "    deletion =1\n",
    "    replace = 2\n",
    "    \n",
    "    #Construct & initialize DP table\n",
    "    dp_table = DP_table(word1,word2)\n",
    "    for i in range(2,(len(word2)+2)):\n",
    "        for j in range(2,(len(word1)+2)):\n",
    "            if dp_table[0][j] ==  dp_table[i][0]:\n",
    "                dp_table[i][j] = min((dp_table[i][j-1]+insertion),(dp_table[i-1][j]+deletion),(dp_table[i-1][j-1]))\n",
    "            else:\n",
    "                dp_table[i][j] = min((dp_table[i][j-1]+insertion),(dp_table[i-1][j]+deletion),(dp_table[i-1][j-1]+replace))\n",
    "            \n",
    "   \n",
    "    return dp_table\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def comp_closest_words(word):\n",
    "    \n",
    "    '''Compute a number(CLOSEST_WORDS global variable) of closest words(based on Levenstein distance)\n",
    "    of the parsed word. The set of unigrams is used as vocabulary \n",
    "    \n",
    "    '''\n",
    "    pq = pqdict()\n",
    "    for voc_word in VOCABULARY:\n",
    "        dist = compute_distance(word,voc_word)\n",
    "        pq.update({voc_word:dist[-1][-1]})\n",
    "    closest_words = {}       \n",
    "    for idx in range(CLOSEST_WORDS):\n",
    "        closest_words.update({pq.popitem()})\n",
    "    return closest_words\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def viterbi_corrector(sentence):\n",
    "    \n",
    "    '''\n",
    "    Implements a viterbi decoder for context-sensitive spelling corrector. The recursive formula of the\n",
    "    viterbi decoder is: V(k) = LD(k) * max(P(wk|wk-1)* Vk-1(wk-1)) , where LD is the Levestein distance\n",
    "    from the sentence's words and P(wk|wk-1) the log propability of bigram(wk,wk-1)\n",
    "    '''\n",
    "    \n",
    "    Words = np.empty([CLOSEST_WORDS, len(sentence)],dtype = 'U10000')\n",
    "    LD_Distance = np.empty([CLOSEST_WORDS, len(sentence)],dtype = int)\n",
    "\n",
    "    for idx in range(len(sentence)):\n",
    "        closest_words = comp_closest_words(sentence[idx])\n",
    "        jdx = 0\n",
    "        for k,v in closest_words.items():\n",
    "            Words[jdx,idx] = k\n",
    "            LD_Distance[jdx,idx] = v\n",
    "            jdx +=1\n",
    "\n",
    "    Vit = np.empty([CLOSEST_WORDS, len(sentence)],dtype = 'f')\n",
    "    Vit_trace = np.empty([CLOSEST_WORDS, len(sentence)-1],dtype = int)\n",
    "\n",
    "    for jdx in range(len(sentence)):\n",
    "        for idx in range(CLOSEST_WORDS):\n",
    "                if (jdx ==0):\n",
    "                    #Added +1 to LD distance prevents original sentence's words(LD=0) to always be chosen as max possible\n",
    "                    Vit[idx,jdx]= bigram_logprob_Mod_KN((\"<s>\",Words[idx,jdx])) * (LD_Distance[idx,jdx]+1)\n",
    "                else:\n",
    "                    bigram_logprob_Mod_KN((\"<s>\",Words[idx,jdx]))\n",
    "                    maxprob = -float(\"inf\")\n",
    "                    trace = 0\n",
    "                    for idx2 in range(CLOSEST_WORDS):\n",
    "                        prob = Vit[idx2,jdx-1] +(bigram_logprob_Mod_KN((Words[idx2,jdx-1],Words[idx,jdx]))\n",
    "                                                 * (LD_Distance[idx,jdx]+1))\n",
    "                        if (prob > maxprob):\n",
    "                            maxprob = prob\n",
    "                            trace = idx2\n",
    "                    Vit[idx,jdx] = maxprob\n",
    "                    Vit_trace[idx,jdx-1] = trace\n",
    "\n",
    "    max_endprob = -float(\"inf\")\n",
    "    end_trace = 0\n",
    "    for idx in range(CLOSEST_WORDS):                        \n",
    "        prob = Vit[idx,-1] +(bigram_logprob_Mod_KN((Words[idx,-1],\"<s>\"))  * (LD_Distance[idx,-1]+1))\n",
    "        if (prob > maxprob):\n",
    "            max_endprob = prob\n",
    "            end_trace = idx\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    words_idx = []\n",
    "    words_idx.append(end_trace)\n",
    "    idx = len(sentence)-2\n",
    "    while (idx >= 0):\n",
    "        words_idx.append(Vit_trace[words_idx[-1],idx])\n",
    "        idx -=1\n",
    "\n",
    "    words_idx = list(reversed(words_idx))\n",
    "\n",
    "\n",
    "    corrected_sentence = []\n",
    "    for idx in range(len(sentence)):\n",
    "        corrected_sentence.append(Words[words_idx[idx],idx])\n",
    "    return corrected_sentence\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def greedy_corrector(sentence):\n",
    "    \n",
    "    '''\n",
    "    Implements a baseline context-sensitive spelling corrector. The spelling corrector\n",
    "    is going to take into account only the max P(wk|wk-1) and the Levestein distance\n",
    "    from the sentence's words(greedy choice). The formula is  P(k) = LD(k) * max(P(wk|wk-1)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    Words = np.empty([CLOSEST_WORDS, len(sentence)],dtype = 'U10000')\n",
    "    LD_Distance = np.empty([CLOSEST_WORDS, len(sentence)],dtype = int)\n",
    "\n",
    "    for idx in range(len(sentence)):\n",
    "        closest_words = comp_closest_words(sentence[idx])\n",
    "        jdx = 0\n",
    "        for k,v in closest_words.items():\n",
    "            Words[jdx,idx] = k\n",
    "            LD_Distance[jdx,idx] = v\n",
    "            jdx +=1\n",
    "\n",
    "    corrected_sentence = []\n",
    "    maxprob = -float(\"inf\")\n",
    "    index = 0\n",
    "    for idx in range(CLOSEST_WORDS):\n",
    "        prob = bigram_logprob_Mod_KN((\"<s>\",Words[idx,0])) * (LD_Distance[idx,0]+1)\n",
    "        if prob > maxprob:\n",
    "            prob = maxprob\n",
    "            index = idx\n",
    "    corrected_sentence.append(Words[index,0])\n",
    "    \n",
    "    for jdx in range(1,len(sentence)):\n",
    "        maxprob = -float(\"inf\")\n",
    "        index = 0\n",
    "        for idx in range(CLOSEST_WORDS):\n",
    "            prob = bigram_logprob_Mod_KN((corrected_sentence[-1],Words[idx,jdx])) * (LD_Distance[idx,jdx]+1)\n",
    "            if prob > maxprob:\n",
    "                prob = maxprob\n",
    "                index = idx\n",
    "        corrected_sentence.append(Words[index,jdx])\n",
    "        \n",
    "    \n",
    "    return corrected_sentence\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import random\n",
    "\n",
    "TEST_SIZE = 500\n",
    "\n",
    "def perword_accuracy(sentence,corr_sentence):\n",
    "    \n",
    "    #calculate per-word accuracy of the corrected sentence\n",
    "    assert(len(sentence) == len(corr_sentence)),\"error the length of correct and corrected sentence must be equal\"\n",
    "    acc = 0\n",
    "    for idx in range(len(sentence)):\n",
    "        if sentence[idx] == corr_sentence[idx]:\n",
    "            acc +=1\n",
    "            \n",
    "    return acc/len(sentence)\n",
    "    \n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "    \n",
    "def insert_errors(sentence):\n",
    "    \n",
    "    #Indroduce random spelling errors to a given sentence\n",
    "    letters = list(string.ascii_lowercase)\n",
    "    new_sentence = []\n",
    "    for idx in range(len(sentence)):\n",
    "        # add spelling error with 0.5 prob for each word\n",
    "        dice = random.randint(1,2)\n",
    "        if (dice == 1):\n",
    "            word = list(sentence[idx])\n",
    "            if (len(word) < 5):\n",
    "                word.append( random.choice(letters))\n",
    "            else:\n",
    "                idx1 = random.randint(0,len(word)-1)\n",
    "                idx2 = random.randint(0,len(word)-1)\n",
    "                word[idx1] = random.choice(letters)\n",
    "                word[idx2] = random.choice(letters)\n",
    "            word = \"\".join(word)\n",
    "            new_sentence.append(word)\n",
    "            \n",
    "        else:\n",
    "             new_sentence.append(sentence[idx])\n",
    "       \n",
    "    return new_sentence\n",
    "              \n",
    "\n",
    "#Select randomly a sample(of size TEST_SIZE) from test data\n",
    "test_sample =random.sample(test, TEST_SIZE)\n",
    "\n",
    "per_word_acc_viterbi = 0\n",
    "per_word_acc_greedy = 0\n",
    "per_word_acc_baseline = 0\n",
    "\n",
    "for sent in test_sample:\n",
    "    n_sen = insert_errors(sent)\n",
    "    per_word_acc_viterbi+= perword_accuracy(sent,viterbi_corrector(n_sen))\n",
    "    per_word_acc_greedy+= perword_accuracy(sent,greedy_corrector(n_sen))\n",
    "    per_word_acc_baseline+= perword_accuracy(sent,n_sen)\n",
    "    \n",
    "avg_per_word_acc_viterbi = per_word_acc_viterbi/TEST_SIZE\n",
    "avg_per_word_acc_greedy = per_word_acc_greedy/TEST_SIZE\n",
    "avg_per_word_acc_baseline = per_word_acc_baseline/TEST_SIZE\n",
    "\n",
    "print(\"Viterbi average per word accuracy: \", avg_per_word_acc_viterbi)\n",
    "print(\"Greedy average per word accuracy: \", avg_per_word_acc_greedy)\n",
    "print(\"Baseline average per word accuracy: \", avg_per_word_acc_baseline)\n",
    "\n",
    "\n",
    "# Viterbi average per word accuracy:  0.8212886189248795\n",
    "# Greedy average per word accuracy:  0.14442630786640534\n",
    "# Baseline average per word accuracy:  0.5021541453981966"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
